{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Python Exercise 11 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accenni alle Reti Neurali \n",
    "\n",
    "### Neurone Artificiale\n",
    "\n",
    "Modello matematico molto semplificato del neurone biologico. Ad ogni input $x_{i}$ è associato un peso $w_{i}$ con valore positivo o negativo per eccitare o inibire il neurone. \n",
    "\n",
    "#### Algoritmo del neurone\n",
    "\n",
    "- 1 Caricare i valori degli input $x_{i}$ e dei pesi $w_{i}$\n",
    "- 2 Calcolare la somma dei valori input pesata con i relativi pesi\n",
    "- 3 Calcolare il valore della funzione di attivazione g con il risultato della somma pesata\n",
    "- 4 L'output del neurone $y$ è il risultato della funzione di attivazione\n",
    "\n",
    "$$y(x) = g\\biggr(\\displaystyle\\sum_{i=1}^d w_{i}x_{i} + w_{0}\\biggr)$$\n",
    "\n",
    "#### Funzione di attivazione \n",
    "\n",
    "Determina la risposta del neurone\n",
    "\n",
    "### Apprendimento della Rete\n",
    "\n",
    "La programmazione serve solo per creare la rete e l'algoritmo di apprendimento (deve capire come comportarsi con l'input che riceve).\n",
    "\n",
    "I pesi $w_{i}$, indispensabili da ottimizzare per ottenere un buon risultato, vengono scelti mediante le seguenti tecniche algoritmiche:\n",
    "\n",
    "-  #### Funzione di costo .\n",
    "    Rapprsenta la \"perdita\" in termini di efficacia predittiva, determinata dal modello rispetto ai valori reali che, in fase successiva, sono ad esso rapportati. Qusta funzione dipende dai parametri fondamentali (pesi) e tende a zero quanto più l'output del modello si avvicina ai valori di \"testing\". L'obbiettivo è quindi quello di minimizzare la funzione di costo.\n",
    "\n",
    "- #### Optimizer. \n",
    "    Funzione che adatta i pesi, durante la procedura di training, per arrivare a minimizzare la funzione di costo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noi ci occuperemo di reti ad *Apprendimento Supervisionato*. Vengo cioè presentati al computer degli input di esempio ed i relativi output desiderati, con lo scopo di apprendere una regola generale in grado di mappare gli input negli output.\n",
    "\n",
    "Qui sotto verrà affrontato un problema di *regressione*. L'output ha un dominio continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "Esplorare come il codice sulla regressione lineare dipende dal numero di epoche, $N_{\\mathrm{epochs}}$, dai numeri di punti, $N_{\\mathrm{train}}$ e dal rumore $\\sigma$. Migliorare il risultato operando su questi parametri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sin\n",
    "import tensorflow as tf\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tensorflow import keras\n",
    "from keras import optimizers, losses, metrics\n",
    "from keras import activations\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "# target parameters of f(x) = m*x + b\n",
    "m = 2 # slope\n",
    "b = 1 # intersect\n",
    "\n",
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_train = np.random.uniform(-1, 1, 500)\n",
    "x_valid = np.random.uniform(-1, 1, 50)\n",
    "x_valid.sort()\n",
    "y_target = m * x_valid + b # ideal (target) linear function\n",
    "\n",
    "sigma = 0.2 # noise standard deviation, for the moment it is absent\n",
    "y_train = np.random.normal(m * x_train + b, sigma) # actual measures from which we want to guess regression parameters\n",
    "y_valid = np.random.normal(m * x_valid + b, sigma)\n",
    "\n",
    "\n",
    "plt.plot(x_valid, y_target)\n",
    "plt.scatter(x_valid, y_valid, color='r')\n",
    "plt.grid(True); plt.show()\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "\n",
    "# compile the model choosing optimizer, loss and metrics objects\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train, \n",
    "          batch_size=32, epochs=50,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(x_valid, y_valid))\n",
    "\n",
    "score = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(x_valid, y_target, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Provare ad estendere il modello per un polinomio del terzo ordine:\n",
    "\n",
    "$$\n",
    "f(x)=4-3x-2x^2+3x^3\n",
    "$$\n",
    "for $x \\in [-1,1]$.\n",
    "\n",
    "Esplorare differenti valori per:\n",
    "\n",
    "- the number of layers\n",
    "- the number of neurons in each layer\n",
    "- the activation function\n",
    "- the optimizer\n",
    "- the loss function\n",
    "\n",
    "Giudicare i modelli NN vedendo quanto bene i fit predicono nuovi valori di test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "c = -3\n",
    "b = -2\n",
    "a = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "x_train = np.random.uniform(-1, 1, 5000)\n",
    "x_valid = np.random.uniform(-1, 1, 500)\n",
    "x_valid.sort()\n",
    "y_target = d + c * x_valid + b * x_valid*x_valid + a * x_valid * x_valid * x_valid\n",
    "\n",
    "sigma = 0.3 ##noise\n",
    "y_train = np.random.normal(d + c * x_train + b * x_train*x_train + a * x_train * x_train * x_train, sigma)\n",
    "y_valid = np.random.normal(d + c * x_valid + b * x_valid*x_valid + a * x_valid * x_valid * x_valid, sigma)\n",
    "\n",
    "plt.plot(x_valid,y_target)\n",
    "plt.scatter(x_valid, y_valid, color = 'r')\n",
    "plt.grid(True); #plt.show()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(9, input_shape=(1,), activation='relu'))#numero di neuroni nel primo \"hidden layer\" e 1 parametro di input\n",
    "model.add(Dense(18, activation='relu'))# dopo il primo layer non devo specificare più la size dell'input\n",
    "model.add(Dense(27, activation='relu'))\n",
    "model.add(Dense(1))#output\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=100, shuffle=True,validation_data=(x_valid,y_valid))\n",
    "\n",
    "score = model.evaluate(x_valid, y_valid, batch_size=32,verbose=1)\n",
    "\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test Accuracy:',score[1])\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best'); plt.show()\n",
    "\n",
    "\n",
    "x_predicted = np.random.uniform(-1, 1, 200)\n",
    "y_predicted = model.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted,color='r')\n",
    "plt.plot(x_valid, y_target)\n",
    "plt.grid(True); plt.show()\n",
    "\n",
    "\n",
    "print(\"Input dataset shape: \", x_valid.shape)\n",
    "# this model maps a 2 dims problem into an 9 dims\n",
    "y_predicted = model.predict(x_valid, batch_size=128)\n",
    "print(\"Predicted results shape: \", y_predicted.shape)\n",
    "print(y_valid[4])\n",
    "print(y_predicted[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi\n",
    "\n",
    "I punti interni al data train si avvicinano bene al valore della funzione mentre quelli esterni si distaccano maggiornamente.\n",
    "\n",
    "Le varie prove che sono state fatte vertevano sul variare numero di neuroni nei layers, variare numero di layers, variare la \"action function\" , l'optimizer, la \"loss function\", e le epoche. \n",
    "\n",
    "Ho visto che: \n",
    "\n",
    "- aumentare il numero di neuroni e di layer aiutava a fittare meglio la curva .\n",
    "- La funzione di attivazione è ricaduta su quella più utilizzata in letteratura (relu, sia per hidden layers che per input layers).\n",
    "- Ricordando che il \"Gradient descend\" è un processo iterativo, ho aumentato il numero di epohe aumentando così la bontà del mio fit (stando attenta a non aumentare troppo le opeche per evitare l'overfit). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "\n",
    "Provare a estendere il modello per fittare $f(x,y) = \\sin(x^2+y^2)$ nel range $x \\in [-3/2,3/2]$ and $y \\in [-3/2,3/2]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_train = 5000\n",
    "n_valid = 500\n",
    "\n",
    "x_train = np.zeros((n_train,2)) #mi costruisco le matrici\n",
    "y_train = np.zeros((n_train,2))\n",
    "\n",
    "x_valid = np.zeros((n_valid,2))\n",
    "y_valid = np.zeros((n_valid,2))\n",
    "\n",
    "y_target = np.zeros(n_valid)\n",
    "y_train = np.zeros(n_train)\n",
    "y_valid = np.zeros(n_valid)\n",
    "\n",
    "for i in range(n_train):\n",
    "\tx_train[i,0] = np.random.uniform(-1.5, 1.5)\n",
    "\tx_train[i,1] = np.random.uniform(-1.5, 1.5)\n",
    "\n",
    "for i in range(n_valid):\n",
    "\tx_valid[i,0] = np.random.uniform(-1.5, 1.5)\n",
    "\tx_valid[i,1] = np.random.uniform(-1.5, 1.5)\n",
    "\n",
    "for i in range(n_valid):\n",
    "\ty_target[i] = np.sin(x_valid[i,0]*x_valid[i,0] + x_valid[i,1]*x_valid[i,1])\n",
    "\n",
    "sigma = 0.3 ##noise\n",
    "for i in range(n_train):\n",
    "\ty_train[i] = np.random.normal(np.sin(x_train[i,0]*x_train[i,0] + x_train[i,1]*x_train[i,1]), sigma)\n",
    "\n",
    "for i in range(n_valid):\n",
    "\ty_valid[i] = np.random.normal(np.sin(x_valid[i,0]*x_valid[i,0] + x_valid[i,1]*x_valid[i,1]), sigma)\n",
    "\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_trisurf(x_valid[:,0],x_valid[:,1], y_valid,cmap='viridis', edgecolor = 'none')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.view_init(30,20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(12, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "#relu = activations.relu(x_train,alpha=0.5)\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "#sgd = optimizers.SGD(lr=0.01, decay = 1e-6, momentum=0.9, nesterov=True)\n",
    "#keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=50, epochs=100, shuffle=True,validation_data=(x_valid,y_valid))\n",
    "\n",
    "score = model.evaluate(x_valid, y_valid, batch_size=50,verbose=1)\n",
    "\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test Accuracy:',score[1])\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best'); plt.show()\n",
    "\n",
    "x_predicted = np.zeros((500,2))\n",
    "\n",
    "for i in range(500):\n",
    "\tx_predicted[i,0] = np.random.uniform(-1.5, 1.5)\n",
    "\tx_predicted[i,1] = np.random.uniform(-1.5, 1.5)\n",
    "\n",
    "y_predicted = model.predict(x_predicted)\n",
    "\n",
    "print (y_predicted[:,0].shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(x_predicted[:,0],x_predicted[:,1], y_predicted[:,0], c = y_predicted[:,0], marker = '.')\n",
    "ax.plot_trisurf(x_valid[:,0],x_valid[:,1], y_target,cmap='viridis', color = 'none')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.view_init(30,20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Input dataset shape: \", x_valid.shape)\n",
    "# this model maps a 2 dims problem into a 1 dims\n",
    "y_predicted = model.predict(x_valid, batch_size=128)\n",
    "print(\"Predicted results shape: \", y_predicted.shape)\n",
    "print(y_valid[2])\n",
    "print(y_predicted[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
